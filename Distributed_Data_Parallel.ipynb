{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group"
      ],
      "metadata": {
        "id": "aWXZnS7THQCa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "H0JpdZNIGHMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset generates size number of samples, where each sample is a tuple:\n",
        "\n",
        "\n",
        "\n",
        "*   A 20-dimensional input tensor (torch.rand(20))\n",
        "*   A 1-dimensional target/output tensor (torch.rand(1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "It's useful for testing models quickly without needing real data."
      ],
      "metadata": {
        "id": "yChmrRjDGS2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTrainDataset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.data = [(torch.rand(20), torch.randint(0, 2, (1,)).item()) for _ in range(size)]  # binary classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.data[index]\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "SM_nADVeGKTt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single GPU/ Non- distributed Training"
      ],
      "metadata": {
        "id": "l6IcmYQEFtbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The single-GPU training section defines a simple training workflow that runs on a single device—either CPU or a single CUDA-enabled GPU. It includes a custom dataset class (MyTrainDataset) that generates random input-target pairs for binary classification. A Trainer class encapsulates the training logic: it moves data and models to the selected device, performs forward and backward passes, computes binary cross-entropy loss, and updates weights using stochastic gradient descent (SGD). A checkpoint is saved periodically to disk to preserve model state. This setup is ideal for debugging or small-scale training without needing distributed processing, and it’s straightforward to run in notebooks or local machines with limited GPU availability."
      ],
      "metadata": {
        "id": "jfUmNRc8bSrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_data, optimizer, device, save_every):\n",
        "        self.device = device\n",
        "        self.model = model.to(device)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.binary_cross_entropy_with_logits(output.squeeze(), targets.float())\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[{self.device}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.device)\n",
        "            targets = targets.to(self.device)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        PATH = \"checkpoint.pt\"\n",
        "        torch.save(self.model.state_dict(), PATH)\n",
        "        print(f\"Epoch {epoch} | Checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n"
      ],
      "metadata": {
        "id": "x954tmtIHYm1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines a training loop for standard (non-distributed) PyTorch training. This:\n",
        "\n",
        "\n",
        "\n",
        "*   Moves data and model to GPU/CPU (device)\n",
        "*   Calculates loss\n",
        "*   Performs backprop and optimizer step\n",
        "*   Saves checkpoint every few epochs"
      ],
      "metadata": {
        "id": "KQN1Pp3Ka4z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "def load_train_objs():\n",
        "    train_set = MyTrainDataset(2048)\n",
        "    model = torch.nn.Linear(20, 1)  # Output logit for binary classification\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "def prepare_dataloader(dataset, batch_size):\n",
        "    return DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=True)"
      ],
      "metadata": {
        "id": "c8qsBIftHceO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "total_epochs = 5\n",
        "save_every = 1\n",
        "batch_size = 32\n",
        "\n",
        "dataset, model, optimizer = load_train_objs()\n",
        "train_data = prepare_dataloader(dataset, batch_size)\n",
        "trainer = Trainer(model, train_data, optimizer, device, save_every)\n",
        "trainer.train(total_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWBQniG4HgyQ",
        "outputId": "578e7229-a2bc-4643-d0ca-d820e4c00366"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cuda] Epoch 0 | Batchsize: 32 | Steps: 64\n",
            "Epoch 0 | Checkpoint saved at checkpoint.pt\n",
            "[cuda] Epoch 1 | Batchsize: 32 | Steps: 64\n",
            "Epoch 1 | Checkpoint saved at checkpoint.pt\n",
            "[cuda] Epoch 2 | Batchsize: 32 | Steps: 64\n",
            "Epoch 2 | Checkpoint saved at checkpoint.pt\n",
            "[cuda] Epoch 3 | Batchsize: 32 | Steps: 64\n",
            "Epoch 3 | Checkpoint saved at checkpoint.pt\n",
            "[cuda] Epoch 4 | Batchsize: 32 | Steps: 64\n",
            "Epoch 4 | Checkpoint saved at checkpoint.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-GPU"
      ],
      "metadata": {
        "id": "A1c6T7vpHrAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The multi-GPU training section leverages PyTorch's DistributedDataParallel (DDP) framework to scale training across multiple GPUs in parallel. The setup begins with a ddp_setup() function that initializes the process group and assigns each training process to a dedicated GPU. A DDP-compatible Trainer class is then used, which wraps the model with torch.nn.parallel.DistributedDataParallel and distributes data loading using DistributedSampler, ensuring that each process sees a unique shard of the dataset. The set_epoch() call ensures data is reshuffled differently at each epoch across processes. Training proceeds independently on each GPU, with synchronization handled by DDP under the hood. Checkpoints are saved only by the process with rank=0 to avoid race conditions. This structure enables efficient training of large models on machines with multiple GPUs, such as AWS EC2 instances like p3.8xlarge."
      ],
      "metadata": {
        "id": "kDZArFoBbZ5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your model contains any BatchNorm layers, it needs to be converted to SyncBatchNorm to sync the running stats of BatchNorm layers across replicas.\n",
        "\n",
        "Use the helper function torch.nn.SyncBatchNorm.convert_sync_batchnorm(model) to convert all BatchNorm layers in the model to SyncBatchNorm."
      ],
      "metadata": {
        "id": "F_GFBtkfNDBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DDP Setup ---\n",
        "def ddp_setup(rank, world_size):\n",
        "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
        "    torch.cuda.set_device(rank)\n",
        "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)"
      ],
      "metadata": {
        "id": "QNEoNUAyKivn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Trainer ---\n",
        "class Trainer:\n",
        "    def __init__(self, model, train_data, optimizer, gpu_id, save_every):\n",
        "        self.gpu_id = gpu_id\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "        self.model = DDP(model, device_ids=[gpu_id])\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source).squeeze()\n",
        "        loss = F.binary_cross_entropy_with_logits(output, targets.float())\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        self.train_data.sampler.set_epoch(epoch) # call this additional line at every epoch\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        if self.gpu_id == 0:\n",
        "            ckp = self.model.module.state_dict()\n",
        "            PATH = \"checkpoint.pt\"\n",
        "            torch.save(ckp, PATH)\n",
        "            print(f\"Epoch {epoch} | Checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)"
      ],
      "metadata": {
        "id": "kYIs9CvWKjiX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the set_epoch() method on the DistributedSampler at the beginning of each epoch is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be used in each epoch."
      ],
      "metadata": {
        "id": "AbuVEjNYNmRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Launch ---\n",
        "def train_ddp(rank, world_size, total_epochs, save_every, batch_size):\n",
        "    ddp_setup(rank, world_size)\n",
        "    dataset = MyTrainDataset(2048)\n",
        "    sampler = DistributedSampler(dataset)\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, pin_memory=True)\n",
        "\n",
        "    model = torch.nn.Linear(20, 1)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "    trainer = Trainer(model, train_loader, optimizer, gpu_id=rank, save_every=save_every)\n",
        "    trainer.train(total_epochs)\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "id": "ePnWbUVqKoFy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DistributedSampler chunks the input data across all distributed processes.\n",
        "\n",
        "The DataLoader combines a dataset and a\n",
        "sampler, and provides an iterable over the given dataset.\n",
        "\n",
        "Each process will receive an input batch of 32 samples; the effective batch size is 32 * nprocs, or 128 when using 4 GPUs."
      ],
      "metadata": {
        "id": "PtHY0r-SNfLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Config ---\n",
        "world_size = 1  # Set to >1 for multi-GPU when running on AWS\n",
        "total_epochs = 3\n",
        "save_every = 1\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "mqZo0iTrKpAG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "world_size is the number of processes across the training job. For GPU training, this corresponds to the number of GPUs in use, and each process works on a dedicated GPU."
      ],
      "metadata": {
        "id": "3dMNQ0hTN8y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ddp(rank=0, world_size=1, total_epochs=3, save_every=1, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQEFw_njK-y5",
        "outputId": "5d272302-43fe-4097-8a41-36cc07263388"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GPU0] Epoch 0 | Batchsize: 32 | Steps: 64\n",
            "Epoch 0 | Checkpoint saved at checkpoint.pt\n",
            "[GPU0] Epoch 1 | Batchsize: 32 | Steps: 64\n",
            "Epoch 1 | Checkpoint saved at checkpoint.pt\n",
            "[GPU0] Epoch 2 | Batchsize: 32 | Steps: 64\n",
            "Epoch 2 | Checkpoint saved at checkpoint.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only need to save model checkpoints from one process. Without this condition, each process would save its copy of the identical mode."
      ],
      "metadata": {
        "id": "vcqannOLNwFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Spawn (when you have access to multiple GPUs. Else use the command above.) ---\n",
        "import torch.multiprocessing as mp #PyTorch wrapper around Python’s native multiprocessing\n",
        "mp.spawn(\n",
        "    train_ddp,\n",
        "    args=(world_size, total_epochs, save_every, batch_size),\n",
        "    nprocs=world_size\n",
        ")"
      ],
      "metadata": {
        "id": "LQmeS1iTLJ4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab does not fully support torch.multiprocessing.spawn(), especially when it tries to create new Python processes inside a notebook cell.\n",
        "\n",
        "This happens even when world_size=1 because spawn() still starts a subprocess, which doesn’t work reliably in Colab's Jupyter runtime."
      ],
      "metadata": {
        "id": "8cPEmam0LPhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fault Tolerance in Distributed Training with torchrun"
      ],
      "metadata": {
        "id": "2PUeovkILRfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fault-tolerant training section builds on the DDP foundation and introduces support for elastic training via torchrun and checkpoint-based resumption. Instead of manually specifying ranks, it reads the LOCAL_RANK environment variable set by torchrun, enabling seamless integration with PyTorch's built-in launcher. This version of the Trainer class supports snapshot loading, allowing training to resume from the last saved epoch in the event of a failure or preemption. The snapshot includes both the model's state and the number of epochs already completed. By restoring from these checkpoints, training jobs can continue without restarting from scratch. This approach is critical in cloud environments where interruptions are common or where long training jobs need robust recovery mechanisms."
      ],
      "metadata": {
        "id": "XNxNK2Ywbfn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch offers a utility called torchrun that provides fault-tolerance and elastic training. When a failure occurs, torchrun logs the errors and attempts to automatically restart all the processes from the last saved “snapshot” of the training job."
      ],
      "metadata": {
        "id": "uGgn0U4_QHYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DDP Setup ---\n",
        "def ddp_setup():\n",
        "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))  # default to 0 for Colab\n",
        "    torch.cuda.set_device(local_rank)\n",
        "    init_process_group(backend=\"nccl\")"
      ],
      "metadata": {
        "id": "j1VR26NsSUf1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Trainer ---\n",
        "class Trainer:\n",
        "    def __init__(self, model, train_data, optimizer, save_every, snapshot_path):\n",
        "        self.gpu_id = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
        "        self.model = model.to(self.gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "        self.epochs_run = 0\n",
        "        self.snapshot_path = snapshot_path\n",
        "        if os.path.exists(snapshot_path):\n",
        "            print(\"Loading snapshot...\")\n",
        "            self._load_snapshot(snapshot_path)\n",
        "        self.model = DDP(self.model, device_ids=[self.gpu_id])\n",
        "\n",
        "    def _load_snapshot(self, snapshot_path):\n",
        "        loc = f\"cuda:{self.gpu_id}\"\n",
        "        snapshot = torch.load(snapshot_path, map_location=loc)\n",
        "        self.model.load_state_dict(snapshot[\"MODEL_STATE\"])\n",
        "        self.epochs_run = snapshot[\"EPOCHS_RUN\"]\n",
        "        print(f\"Resuming training from snapshot at Epoch {self.epochs_run}\")\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source).squeeze()\n",
        "        loss = F.binary_cross_entropy_with_logits(output, targets.float())\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        self.train_data.sampler.set_epoch(epoch)\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_snapshot(self, epoch):\n",
        "        if self.gpu_id == 0:\n",
        "            snapshot = {\n",
        "                \"MODEL_STATE\": self.model.module.state_dict(),\n",
        "                \"EPOCHS_RUN\": epoch,\n",
        "            }\n",
        "            torch.save(snapshot, self.snapshot_path)\n",
        "            print(f\"Epoch {epoch} | Snapshot saved to {self.snapshot_path}\")\n",
        "\n",
        "    def train(self, max_epochs):\n",
        "        for epoch in range(self.epochs_run, max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
        "                self._save_snapshot(epoch)"
      ],
      "metadata": {
        "id": "Y-dxYaNpPmqo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run Directly in Notebook ---\n",
        "# CONFIGURE HERE\n",
        "total_epochs = 5\n",
        "save_every = 1\n",
        "batch_size = 32\n",
        "snapshot_path = \"snapshot.pt\""
      ],
      "metadata": {
        "id": "PbiObujUScUI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Training\n",
        "ddp_setup()\n",
        "dataset, model, optimizer = MyTrainDataset(2048), torch.nn.Linear(20, 1), torch.optim.SGD(torch.nn.Linear(20, 1).parameters(), lr=1e-3)\n",
        "train_data = DataLoader(dataset, batch_size=batch_size, shuffle=False, sampler=DistributedSampler(dataset), pin_memory=True)\n",
        "trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)\n",
        "trainer.train(total_epochs)\n",
        "destroy_process_group()"
      ],
      "metadata": {
        "id": "CGzRgiR2SlKh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}